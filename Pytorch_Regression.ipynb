{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "boston = load_boston()\n",
    "train_x, test_x, train_y, test_y = train_test_split(boston.data, boston.target, random_state=seed)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "train_x = torch.tensor(scaler.fit_transform(train_x), dtype=torch.float)\n",
    "test_x = torch.tensor(scaler.transform(test_x), dtype=torch.float)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float).view(-1, 1)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "layer_size = train_x.shape[1]\n",
    "lr = 0.01\n",
    "epochs = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class Regression(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(layer_size, 1) # equivalent of wx + b\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_1(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Regression(layer_size) # Defines our parameters for us\n",
    "loss_func = nn.MSELoss() # Define loss func\n",
    "opt = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 600.7393188476562 Test Loss: 451.9837951660156\n",
      "Epoch: 10 Training Loss: 221.6092987060547 Test Loss: 159.03054809570312\n",
      "Epoch: 20 Training Loss: 134.75296020507812 Test Loss: 99.14453887939453\n",
      "Epoch: 30 Training Loss: 110.88875579833984 Test Loss: 85.04891967773438\n",
      "Epoch: 40 Training Loss: 101.0737533569336 Test Loss: 79.29972839355469\n",
      "Epoch: 50 Training Loss: 94.80081176757812 Test Loss: 74.97420501708984\n",
      "Epoch: 60 Training Loss: 89.76229095458984 Test Loss: 71.03230285644531\n",
      "Epoch: 70 Training Loss: 85.40636444091797 Test Loss: 67.41524505615234\n",
      "Epoch: 80 Training Loss: 81.56295013427734 Test Loss: 64.15592956542969\n",
      "Epoch: 90 Training Loss: 78.1500015258789 Test Loss: 61.25789260864258\n",
      "Epoch: 100 Training Loss: 75.11003875732422 Test Loss: 58.699703216552734\n",
      "Epoch: 110 Training Loss: 72.3958969116211 Test Loss: 56.44892501831055\n",
      "Epoch: 120 Training Loss: 69.96705627441406 Test Loss: 54.47059631347656\n",
      "Epoch: 130 Training Loss: 67.78826904296875 Test Loss: 52.731163024902344\n",
      "Epoch: 140 Training Loss: 65.82879638671875 Test Loss: 51.20004653930664\n",
      "Epoch: 150 Training Loss: 64.06177520751953 Test Loss: 49.85004806518555\n",
      "Epoch: 160 Training Loss: 62.46378707885742 Test Loss: 48.65724182128906\n",
      "Epoch: 170 Training Loss: 61.01433563232422 Test Loss: 47.600730895996094\n",
      "Epoch: 180 Training Loss: 59.69554901123047 Test Loss: 46.66232681274414\n",
      "Epoch: 190 Training Loss: 58.49179458618164 Test Loss: 45.82624435424805\n",
      "Epoch: 200 Training Loss: 57.38941955566406 Test Loss: 45.07875442504883\n",
      "Epoch: 210 Training Loss: 56.3764762878418 Test Loss: 44.407981872558594\n",
      "Epoch: 220 Training Loss: 55.44253921508789 Test Loss: 43.803611755371094\n",
      "Epoch: 230 Training Loss: 54.57848358154297 Test Loss: 43.2567253112793\n",
      "Epoch: 240 Training Loss: 53.77631378173828 Test Loss: 42.75960159301758\n",
      "Epoch: 250 Training Loss: 53.029052734375 Test Loss: 42.30555725097656\n",
      "Epoch: 260 Training Loss: 52.33058547973633 Test Loss: 41.88880920410156\n",
      "Epoch: 270 Training Loss: 51.67555618286133 Test Loss: 41.50437545776367\n",
      "Epoch: 280 Training Loss: 51.05928039550781 Test Loss: 41.1479377746582\n",
      "Epoch: 290 Training Loss: 50.47764205932617 Test Loss: 40.81578063964844\n",
      "Epoch: 300 Training Loss: 49.927040100097656 Test Loss: 40.50468826293945\n",
      "Epoch: 310 Training Loss: 49.40431213378906 Test Loss: 40.21189880371094\n",
      "Epoch: 320 Training Loss: 48.906681060791016 Test Loss: 39.935035705566406\n",
      "Epoch: 330 Training Loss: 48.431705474853516 Test Loss: 39.67203903198242\n",
      "Epoch: 340 Training Loss: 47.97724914550781 Test Loss: 39.42116165161133\n",
      "Epoch: 350 Training Loss: 47.541412353515625 Test Loss: 39.180885314941406\n",
      "Epoch: 360 Training Loss: 47.12254333496094 Test Loss: 38.94990921020508\n",
      "Epoch: 370 Training Loss: 46.71916961669922 Test Loss: 38.72712326049805\n",
      "Epoch: 380 Training Loss: 46.32999801635742 Test Loss: 38.511573791503906\n",
      "Epoch: 390 Training Loss: 45.95389175415039 Test Loss: 38.30245590209961\n",
      "Epoch: 400 Training Loss: 45.58982849121094 Test Loss: 38.09906005859375\n",
      "Epoch: 410 Training Loss: 45.23692321777344 Test Loss: 37.90080261230469\n",
      "Epoch: 420 Training Loss: 44.89437484741211 Test Loss: 37.70716857910156\n",
      "Epoch: 430 Training Loss: 44.56147003173828 Test Loss: 37.517738342285156\n",
      "Epoch: 440 Training Loss: 44.23758316040039 Test Loss: 37.33213806152344\n",
      "Epoch: 450 Training Loss: 43.922149658203125 Test Loss: 37.15005874633789\n",
      "Epoch: 460 Training Loss: 43.61466979980469 Test Loss: 36.97124099731445\n",
      "Epoch: 470 Training Loss: 43.3146858215332 Test Loss: 36.79545593261719\n",
      "Epoch: 480 Training Loss: 43.02179718017578 Test Loss: 36.62252426147461\n",
      "Epoch: 490 Training Loss: 42.73563766479492 Test Loss: 36.452274322509766\n",
      "Epoch: 500 Training Loss: 42.45588302612305 Test Loss: 36.284584045410156\n",
      "Epoch: 510 Training Loss: 42.18221664428711 Test Loss: 36.11933898925781\n",
      "Epoch: 520 Training Loss: 41.91438293457031 Test Loss: 35.95642852783203\n",
      "Epoch: 530 Training Loss: 41.652130126953125 Test Loss: 35.7957878112793\n",
      "Epoch: 540 Training Loss: 41.39522933959961 Test Loss: 35.6373405456543\n",
      "Epoch: 550 Training Loss: 41.14347457885742 Test Loss: 35.48103332519531\n",
      "Epoch: 560 Training Loss: 40.89667892456055 Test Loss: 35.326812744140625\n",
      "Epoch: 570 Training Loss: 40.654659271240234 Test Loss: 35.174644470214844\n",
      "Epoch: 580 Training Loss: 40.41726303100586 Test Loss: 35.024478912353516\n",
      "Epoch: 590 Training Loss: 40.18433380126953 Test Loss: 34.87629699707031\n",
      "Epoch: 600 Training Loss: 39.955726623535156 Test Loss: 34.73006820678711\n",
      "Epoch: 610 Training Loss: 39.731319427490234 Test Loss: 34.58576965332031\n",
      "Epoch: 620 Training Loss: 39.510982513427734 Test Loss: 34.44337463378906\n",
      "Epoch: 630 Training Loss: 39.29460525512695 Test Loss: 34.30286407470703\n",
      "Epoch: 640 Training Loss: 39.082069396972656 Test Loss: 34.164222717285156\n",
      "Epoch: 650 Training Loss: 38.8732795715332 Test Loss: 34.02743148803711\n",
      "Epoch: 660 Training Loss: 38.66814041137695 Test Loss: 33.892478942871094\n",
      "Epoch: 670 Training Loss: 38.466552734375 Test Loss: 33.75934600830078\n",
      "Epoch: 680 Training Loss: 38.2684326171875 Test Loss: 33.628013610839844\n",
      "Epoch: 690 Training Loss: 38.073692321777344 Test Loss: 33.498477935791016\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    model.train()\n",
    "    pred = model(train_x)\n",
    "    loss = loss_func(pred, train_y)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # Validate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(test_x)\n",
    "        val_loss = loss_func(val_pred, test_y)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} Training Loss: {loss.item()} Test Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
