{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pytorch](images/pytorch_logo.png)\n",
    "# Regression in Pytorch with all the bells and whistles\n",
    "Pytorch lets us skip a bunch of math, but that's only part of why it's heavily used in Deep Learning. It is after all a Deep Learning framework, not a maths framework!\n",
    "\n",
    "Let's redo our simple Linear Regression, but this time we use all the bells and whistles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We are using the built-in sklearn dataset Boston House Prices.\n",
    "\n",
    "Our goal is to predict the median price of a home in a given town from a number of features, such as Crime Rate, Property Tax Rate, amount of Industry etc.\n",
    "\n",
    "It's generally a good idea to scale our data, so we use Sklearn's MinMax scaler to scale our values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "boston = load_boston()\n",
    "train_x, test_x, train_y, test_y = train_test_split(boston.data, boston.target, random_state=seed)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "train_x = torch.tensor(scaler.fit_transform(train_x), dtype=torch.float)\n",
    "test_x = torch.tensor(scaler.transform(test_x), dtype=torch.float)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float).view(-1, 1)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters\n",
    "\n",
    "We have some hyperparameters to set, as well as some numbers we need to know upfront.\n",
    "\n",
    "`layer_size` --> We need to know how many input variables there are, so we can create an equivalent number of weights\n",
    "\n",
    "`lr` --> Aka learning rate.\n",
    "When we take a step in our gradient descent, we multiply by this factor, so we don't take too big or too large a step. \n",
    "\n",
    "`epochs` --> How many times should we keep stepping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "layer_size = train_x.shape[1]\n",
    "lr = 0.05\n",
    "epochs = 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model\n",
    "We get two new toys here:\n",
    "\n",
    "- `nn.Module`\n",
    "\n",
    "Now we can define our Model simply by inheriting from `nn.module`. We define a forward function to use for the forward pass.\n",
    "\n",
    "- `nn.Linear`\n",
    "\n",
    "We can now use these built-in layers such as `nn.Linear` which handle all the parameters for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class Regression(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(layer_size, 1) # equivalent of wx + b\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_1(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function and optimizer\n",
    "Pytorch predefines a number of loss functions for me, so I can just use those directly.\n",
    "Another new features is the SGD optimizer. Until now we have been updating the weights \"by hand\" in a fairly naive fashion. There are many ways of updating our weights and Pytorch provides implementations for most of these. SGD is the closest to what we've been doing so far, so we use that to handle our weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Regression(layer_size) # Defines our parameters for us\n",
    "loss_func = nn.MSELoss() # Define loss func\n",
    "opt = optim.SGD(model.parameters(), lr=lr) # Tell the optimizer what parameters to keep track of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    model.train() # Put the model into train mode\n",
    "    pred = model(train_x) # Our model acts like a function!\n",
    "    loss = loss_func(pred, train_y)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward() # The magic bit\n",
    "    opt.step() # A new magic bit\n",
    "    opt.zero_grad() # Gotta reset the gradients to zero, so they don't accumulate\n",
    "    \n",
    "    # Validate model\n",
    "    model.eval() # Put the model into evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(test_x)\n",
    "        val_loss = loss_func(val_pred, test_y) # Calculate validation loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} Training Loss: {loss.item()} Test Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
