{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pytorch](images/pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in PyTorch\n",
    "Let's reimplement our Deep Learning network, but this time we use all the toys!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9dedfc1d70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "boston = load_boston()\n",
    "train_x, test_x, train_y, test_y = train_test_split(boston.data, boston.target, random_state=seed)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_x = torch.tensor(scaler.fit_transform(train_x), dtype=torch.float)\n",
    "test_x = torch.tensor(scaler.transform(test_x), dtype=torch.float)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float).view(-1, 1)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "layer_size = train_x.shape[1]\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "hidden_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model\n",
    "We get two new toys here:\n",
    "\n",
    "- `nn.Module`\n",
    "\n",
    "Now we can define our Model simply by inheriting from `nn.module`. We define a forward function to use for the forward pass.\n",
    "\n",
    "- `nn.Linear`\n",
    "\n",
    "We can now use these built-in layers such as `nn.Linear` which handle all the parameters for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layer_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(layer_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.relu(self.l2(x))\n",
    "        return self.l3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining optimizer and loss func\n",
    "- `nn.optim`\n",
    "\n",
    "We have access to all the built-in optimizers which handle all the weight updating for us!\n",
    "\n",
    "- `nn.MSELoss`\n",
    "\n",
    "We don't have to write out our loss function if we want to use one of the standard ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layer_size, hidden_size)\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 11.038641929626465 Validation Loss: 14.454541206359863\n",
      "Epoch: 10 Train Loss: 10.317594528198242 Validation Loss: 14.632396697998047\n",
      "Epoch: 20 Train Loss: 10.982763290405273 Validation Loss: 14.438754081726074\n",
      "Epoch: 30 Train Loss: 10.2835693359375 Validation Loss: 14.61897087097168\n",
      "Epoch: 40 Train Loss: 10.997759819030762 Validation Loss: 14.460289001464844\n",
      "Epoch: 50 Train Loss: 10.40207290649414 Validation Loss: 14.705093383789062\n",
      "Epoch: 60 Train Loss: 10.72649097442627 Validation Loss: 14.412620544433594\n",
      "Epoch: 70 Train Loss: 10.209022521972656 Validation Loss: 14.602654457092285\n",
      "Epoch: 80 Train Loss: 10.80444622039795 Validation Loss: 14.46479606628418\n",
      "Epoch: 90 Train Loss: 10.139345169067383 Validation Loss: 14.574867248535156\n",
      "Epoch: 100 Train Loss: 10.811275482177734 Validation Loss: 14.443094253540039\n",
      "Epoch: 110 Train Loss: 10.219914436340332 Validation Loss: 14.64369010925293\n",
      "Epoch: 120 Train Loss: 10.756220817565918 Validation Loss: 14.418624877929688\n",
      "Epoch: 130 Train Loss: 10.326407432556152 Validation Loss: 14.696016311645508\n",
      "Epoch: 140 Train Loss: 10.510229110717773 Validation Loss: 14.493255615234375\n",
      "Epoch: 150 Train Loss: 10.042343139648438 Validation Loss: 14.54001522064209\n",
      "Epoch: 160 Train Loss: 10.221966743469238 Validation Loss: 14.390643119812012\n",
      "Epoch: 170 Train Loss: 9.876657485961914 Validation Loss: 14.48293399810791\n",
      "Epoch: 180 Train Loss: 10.444132804870605 Validation Loss: 14.492528915405273\n",
      "Epoch: 190 Train Loss: 9.99778938293457 Validation Loss: 14.533194541931152\n",
      "Epoch: 200 Train Loss: 10.331578254699707 Validation Loss: 14.435046195983887\n",
      "Epoch: 210 Train Loss: 9.88295841217041 Validation Loss: 14.419205665588379\n",
      "Epoch: 220 Train Loss: 10.586920738220215 Validation Loss: 14.237418174743652\n",
      "Epoch: 230 Train Loss: 9.406147003173828 Validation Loss: 14.442557334899902\n",
      "Epoch: 240 Train Loss: 9.769824028015137 Validation Loss: 14.270111083984375\n",
      "Epoch: 250 Train Loss: 9.87682056427002 Validation Loss: 14.292059898376465\n",
      "Epoch: 260 Train Loss: 9.826895713806152 Validation Loss: 14.369721412658691\n",
      "Epoch: 270 Train Loss: 10.180560111999512 Validation Loss: 14.069083213806152\n",
      "Epoch: 280 Train Loss: 9.561866760253906 Validation Loss: 14.300155639648438\n",
      "Epoch: 290 Train Loss: 10.19619369506836 Validation Loss: 14.051129341125488\n",
      "Epoch: 300 Train Loss: 9.484670639038086 Validation Loss: 14.487003326416016\n",
      "Epoch: 310 Train Loss: 9.588258743286133 Validation Loss: 14.171863555908203\n",
      "Epoch: 320 Train Loss: 9.749516487121582 Validation Loss: 14.38160228729248\n",
      "Epoch: 330 Train Loss: 10.02552604675293 Validation Loss: 14.259944915771484\n",
      "Epoch: 340 Train Loss: 10.101871490478516 Validation Loss: 14.164568901062012\n",
      "Epoch: 350 Train Loss: 9.263505935668945 Validation Loss: 14.105826377868652\n",
      "Epoch: 360 Train Loss: 9.564804077148438 Validation Loss: 14.28189468383789\n",
      "Epoch: 370 Train Loss: 9.848620414733887 Validation Loss: 14.393736839294434\n",
      "Epoch: 380 Train Loss: 10.194881439208984 Validation Loss: 14.084930419921875\n",
      "Epoch: 390 Train Loss: 9.088700294494629 Validation Loss: 13.993206024169922\n",
      "Epoch: 400 Train Loss: 9.217267990112305 Validation Loss: 13.91787338256836\n",
      "Epoch: 410 Train Loss: 9.507614135742188 Validation Loss: 14.254423141479492\n",
      "Epoch: 420 Train Loss: 9.705582618713379 Validation Loss: 13.974629402160645\n",
      "Epoch: 430 Train Loss: 9.356115341186523 Validation Loss: 14.13662052154541\n",
      "Epoch: 440 Train Loss: 9.931378364562988 Validation Loss: 14.316926002502441\n",
      "Epoch: 450 Train Loss: 9.391912460327148 Validation Loss: 13.788999557495117\n",
      "Epoch: 460 Train Loss: 8.950892448425293 Validation Loss: 13.884434700012207\n",
      "Epoch: 470 Train Loss: 10.167084693908691 Validation Loss: 14.112927436828613\n",
      "Epoch: 480 Train Loss: 9.002042770385742 Validation Loss: 13.866348266601562\n",
      "Epoch: 490 Train Loss: 9.391128540039062 Validation Loss: 13.824792861938477\n",
      "Epoch: 500 Train Loss: 8.698795318603516 Validation Loss: 13.93327522277832\n",
      "Epoch: 510 Train Loss: 9.236766815185547 Validation Loss: 14.18481731414795\n",
      "Epoch: 520 Train Loss: 9.65875244140625 Validation Loss: 13.935169219970703\n",
      "Epoch: 530 Train Loss: 8.906693458557129 Validation Loss: 13.82089900970459\n",
      "Epoch: 540 Train Loss: 9.3772611618042 Validation Loss: 14.146271705627441\n",
      "Epoch: 550 Train Loss: 8.926973342895508 Validation Loss: 13.905254364013672\n",
      "Epoch: 560 Train Loss: 8.900550842285156 Validation Loss: 13.959084510803223\n",
      "Epoch: 570 Train Loss: 9.620217323303223 Validation Loss: 14.145211219787598\n",
      "Epoch: 580 Train Loss: 9.35223388671875 Validation Loss: 14.083802223205566\n",
      "Epoch: 590 Train Loss: 9.547056198120117 Validation Loss: 13.947396278381348\n",
      "Epoch: 600 Train Loss: 8.650970458984375 Validation Loss: 13.665985107421875\n",
      "Epoch: 610 Train Loss: 8.64096450805664 Validation Loss: 13.65615463256836\n",
      "Epoch: 620 Train Loss: 8.74018383026123 Validation Loss: 13.860435485839844\n",
      "Epoch: 630 Train Loss: 9.152905464172363 Validation Loss: 14.036310195922852\n",
      "Epoch: 640 Train Loss: 8.9905424118042 Validation Loss: 13.995111465454102\n",
      "Epoch: 650 Train Loss: 8.950754165649414 Validation Loss: 13.810315132141113\n",
      "Epoch: 660 Train Loss: 8.665263175964355 Validation Loss: 13.711236953735352\n",
      "Epoch: 670 Train Loss: 8.602234840393066 Validation Loss: 13.708930969238281\n",
      "Epoch: 680 Train Loss: 8.885756492614746 Validation Loss: 13.852370262145996\n",
      "Epoch: 690 Train Loss: 8.992619514465332 Validation Loss: 13.954044342041016\n",
      "Epoch: 700 Train Loss: 8.892008781433105 Validation Loss: 13.961692810058594\n",
      "Epoch: 710 Train Loss: 8.706626892089844 Validation Loss: 13.725848197937012\n",
      "Epoch: 720 Train Loss: 8.533745765686035 Validation Loss: 13.655113220214844\n",
      "Epoch: 730 Train Loss: 8.793079376220703 Validation Loss: 13.88124942779541\n",
      "Epoch: 740 Train Loss: 8.807025909423828 Validation Loss: 13.822792053222656\n",
      "Epoch: 750 Train Loss: 8.791158676147461 Validation Loss: 13.860238075256348\n",
      "Epoch: 760 Train Loss: 8.828104019165039 Validation Loss: 13.832103729248047\n",
      "Epoch: 770 Train Loss: 8.730304718017578 Validation Loss: 13.855912208557129\n",
      "Epoch: 780 Train Loss: 8.739356994628906 Validation Loss: 13.82903003692627\n",
      "Epoch: 790 Train Loss: 8.616903305053711 Validation Loss: 13.8253755569458\n",
      "Epoch: 800 Train Loss: 8.466018676757812 Validation Loss: 13.606067657470703\n",
      "Epoch: 810 Train Loss: 8.747515678405762 Validation Loss: 13.799701690673828\n",
      "Epoch: 820 Train Loss: 8.577056884765625 Validation Loss: 13.712018966674805\n",
      "Epoch: 830 Train Loss: 8.480217933654785 Validation Loss: 13.782556533813477\n",
      "Epoch: 840 Train Loss: 8.622004508972168 Validation Loss: 13.745863914489746\n",
      "Epoch: 850 Train Loss: 8.635319709777832 Validation Loss: 13.707441329956055\n",
      "Epoch: 860 Train Loss: 8.681087493896484 Validation Loss: 13.79069995880127\n",
      "Epoch: 870 Train Loss: 8.618124961853027 Validation Loss: 13.73796558380127\n",
      "Epoch: 880 Train Loss: 8.612054824829102 Validation Loss: 13.796533584594727\n",
      "Epoch: 890 Train Loss: 8.535564422607422 Validation Loss: 13.701545715332031\n",
      "Epoch: 900 Train Loss: 8.699620246887207 Validation Loss: 13.83571720123291\n",
      "Epoch: 910 Train Loss: 8.808683395385742 Validation Loss: 13.816568374633789\n",
      "Epoch: 920 Train Loss: 8.425415992736816 Validation Loss: 13.776689529418945\n",
      "Epoch: 930 Train Loss: 8.573936462402344 Validation Loss: 13.796695709228516\n",
      "Epoch: 940 Train Loss: 8.62408447265625 Validation Loss: 13.746379852294922\n",
      "Epoch: 950 Train Loss: 8.491255760192871 Validation Loss: 13.773618698120117\n",
      "Epoch: 960 Train Loss: 8.480911254882812 Validation Loss: 13.69329833984375\n",
      "Epoch: 970 Train Loss: 8.363014221191406 Validation Loss: 13.640167236328125\n",
      "Epoch: 980 Train Loss: 8.482995986938477 Validation Loss: 13.773992538452148\n",
      "Epoch: 990 Train Loss: 8.633207321166992 Validation Loss: 13.810816764831543\n",
      "Epoch: 999 Train Loss: 7.810107707977295 Validation Loss: 13.810816764831543\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    pred = model(train_x)\n",
    "    loss = loss_func(pred, train_y)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        val_pred = model(test_x)\n",
    "        val_loss = loss_func(val_pred, test_y)\n",
    "        print(f\"Epoch: {epoch} Train Loss: {loss.item()} Validation Loss: {val_loss.item()}\")\n",
    "print(f\"Epoch: {epoch} Train Loss: {loss.item()} Validation Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
