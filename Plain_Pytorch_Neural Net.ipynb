{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pytorch](images/pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in Pure Pytorch\n",
    "Let's take our Regression and add a hidden layer to it, so we have deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa8f3fa71b0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "boston = load_boston()\n",
    "train_x, test_x, train_y, test_y = train_test_split(boston.data, boston.target, random_state=seed)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_x = torch.tensor(scaler.fit_transform(train_x), dtype=torch.float)\n",
    "test_x = torch.tensor(scaler.transform(test_x), dtype=torch.float)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float).view(-1, 1)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "layer_size = train_x.shape[1]\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "hidden_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def mean_squared_error(y_hat, y):\n",
    "    return ((y_hat - y) ** 2).mean()\n",
    "\n",
    "def relu(x):\n",
    "    return torch.max(torch.tensor(0, dtype=torch.float), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights\n",
    "w_1 = torch.randn(layer_size, hidden_size, requires_grad=True, dtype=torch.float)\n",
    "w_2 = torch.randn(hidden_size, 1, requires_grad=True, dtype=torch.float)\n",
    "b_1 = torch.zeros(1, requires_grad=True, dtype=torch.float)\n",
    "b_2 = torch.zeros(1, requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 363.3018493652344 Test Loss: 109.4739761352539\n",
      "Epoch: 10 Train Loss: 102.40542602539062 Test Loss: 88.30642700195312\n",
      "Epoch: 20 Train Loss: 96.22856903076172 Test Loss: 78.5164794921875\n",
      "Epoch: 30 Train Loss: 77.73710632324219 Test Loss: 66.46686553955078\n",
      "Epoch: 40 Train Loss: 68.07279968261719 Test Loss: 58.34284973144531\n",
      "Epoch: 50 Train Loss: 60.8957633972168 Test Loss: 52.545494079589844\n",
      "Epoch: 60 Train Loss: 56.424381256103516 Test Loss: 48.4997444152832\n",
      "Epoch: 70 Train Loss: 52.62260818481445 Test Loss: 45.20548629760742\n",
      "Epoch: 80 Train Loss: 49.42496109008789 Test Loss: 42.51154708862305\n",
      "Epoch: 90 Train Loss: 46.83031463623047 Test Loss: 40.269187927246094\n",
      "Epoch: 100 Train Loss: 43.64454650878906 Test Loss: 37.87742614746094\n",
      "Epoch: 110 Train Loss: 40.93579864501953 Test Loss: 35.834747314453125\n",
      "Epoch: 120 Train Loss: 38.569766998291016 Test Loss: 34.15199661254883\n",
      "Epoch: 130 Train Loss: 36.78007507324219 Test Loss: 32.84564971923828\n",
      "Epoch: 140 Train Loss: 35.17839813232422 Test Loss: 31.687753677368164\n",
      "Epoch: 150 Train Loss: 33.84564971923828 Test Loss: 30.763757705688477\n",
      "Epoch: 160 Train Loss: 32.82659149169922 Test Loss: 30.016817092895508\n",
      "Epoch: 170 Train Loss: 31.693471908569336 Test Loss: 29.267677307128906\n",
      "Epoch: 180 Train Loss: 30.829042434692383 Test Loss: 28.66791343688965\n",
      "Epoch: 190 Train Loss: 29.978973388671875 Test Loss: 28.093921661376953\n",
      "Epoch: 200 Train Loss: 29.25690269470215 Test Loss: 27.603548049926758\n",
      "Epoch: 210 Train Loss: 29.065502166748047 Test Loss: 27.593473434448242\n",
      "Epoch: 220 Train Loss: 28.844118118286133 Test Loss: 27.442283630371094\n",
      "Epoch: 230 Train Loss: 28.662202835083008 Test Loss: 27.384960174560547\n",
      "Epoch: 240 Train Loss: 27.87550926208496 Test Loss: 26.88053321838379\n",
      "Epoch: 250 Train Loss: 26.999502182006836 Test Loss: 26.36228370666504\n",
      "Epoch: 260 Train Loss: 26.589601516723633 Test Loss: 26.10314178466797\n",
      "Epoch: 270 Train Loss: 26.325166702270508 Test Loss: 25.903583526611328\n",
      "Epoch: 280 Train Loss: 26.060747146606445 Test Loss: 25.727930068969727\n",
      "Epoch: 290 Train Loss: 25.871591567993164 Test Loss: 25.621883392333984\n",
      "Epoch: 300 Train Loss: 25.796676635742188 Test Loss: 25.57123374938965\n",
      "Epoch: 310 Train Loss: 25.678813934326172 Test Loss: 25.479331970214844\n",
      "Epoch: 320 Train Loss: 25.299152374267578 Test Loss: 25.246700286865234\n",
      "Epoch: 330 Train Loss: 24.967744827270508 Test Loss: 25.052059173583984\n",
      "Epoch: 340 Train Loss: 24.896608352661133 Test Loss: 24.982484817504883\n",
      "Epoch: 350 Train Loss: 24.73794174194336 Test Loss: 24.86359214782715\n",
      "Epoch: 360 Train Loss: 24.49480628967285 Test Loss: 24.692161560058594\n",
      "Epoch: 370 Train Loss: 24.21575355529785 Test Loss: 24.52245330810547\n",
      "Epoch: 380 Train Loss: 24.068523406982422 Test Loss: 24.428905487060547\n",
      "Epoch: 390 Train Loss: 23.9085693359375 Test Loss: 24.305484771728516\n",
      "Epoch: 400 Train Loss: 23.86905288696289 Test Loss: 24.295875549316406\n",
      "Epoch: 410 Train Loss: 23.913837432861328 Test Loss: 24.354402542114258\n",
      "Epoch: 420 Train Loss: 23.741546630859375 Test Loss: 24.2255859375\n",
      "Epoch: 430 Train Loss: 23.478397369384766 Test Loss: 24.045530319213867\n",
      "Epoch: 440 Train Loss: 23.206684112548828 Test Loss: 23.897506713867188\n",
      "Epoch: 450 Train Loss: 23.001569747924805 Test Loss: 23.740833282470703\n",
      "Epoch: 460 Train Loss: 22.858684539794922 Test Loss: 23.64667320251465\n",
      "Epoch: 470 Train Loss: 22.680654525756836 Test Loss: 23.551097869873047\n",
      "Epoch: 480 Train Loss: 22.281513214111328 Test Loss: 23.276668548583984\n",
      "Epoch: 490 Train Loss: 22.068220138549805 Test Loss: 23.154111862182617\n",
      "Epoch: 500 Train Loss: 22.104244232177734 Test Loss: 23.192651748657227\n",
      "Epoch: 510 Train Loss: 22.17961311340332 Test Loss: 23.23846435546875\n",
      "Epoch: 520 Train Loss: 22.054134368896484 Test Loss: 23.147319793701172\n",
      "Epoch: 530 Train Loss: 21.727357864379883 Test Loss: 22.96921730041504\n",
      "Epoch: 540 Train Loss: 21.562259674072266 Test Loss: 22.8895206451416\n",
      "Epoch: 550 Train Loss: 21.62500762939453 Test Loss: 22.933378219604492\n",
      "Epoch: 560 Train Loss: 21.629106521606445 Test Loss: 22.938989639282227\n",
      "Epoch: 570 Train Loss: 21.61475944519043 Test Loss: 22.93956756591797\n",
      "Epoch: 580 Train Loss: 21.499521255493164 Test Loss: 22.868438720703125\n",
      "Epoch: 590 Train Loss: 21.336252212524414 Test Loss: 22.779436111450195\n",
      "Epoch: 600 Train Loss: 21.083934783935547 Test Loss: 22.63709831237793\n",
      "Epoch: 610 Train Loss: 20.89557647705078 Test Loss: 22.556180953979492\n",
      "Epoch: 620 Train Loss: 20.96767807006836 Test Loss: 22.604549407958984\n",
      "Epoch: 630 Train Loss: 20.97724151611328 Test Loss: 22.594449996948242\n",
      "Epoch: 640 Train Loss: 20.88610076904297 Test Loss: 22.543657302856445\n",
      "Epoch: 650 Train Loss: 20.56678009033203 Test Loss: 22.370485305786133\n",
      "Epoch: 660 Train Loss: 20.376480102539062 Test Loss: 22.27684211730957\n",
      "Epoch: 670 Train Loss: 20.59617042541504 Test Loss: 22.38471031188965\n",
      "Epoch: 680 Train Loss: 20.650896072387695 Test Loss: 22.403305053710938\n",
      "Epoch: 690 Train Loss: 20.59040069580078 Test Loss: 22.360904693603516\n",
      "Epoch: 700 Train Loss: 20.41244888305664 Test Loss: 22.25287628173828\n",
      "Epoch: 710 Train Loss: 20.302064895629883 Test Loss: 22.19855308532715\n",
      "Epoch: 720 Train Loss: 20.196077346801758 Test Loss: 22.128198623657227\n",
      "Epoch: 730 Train Loss: 20.064855575561523 Test Loss: 22.072622299194336\n",
      "Epoch: 740 Train Loss: 20.197818756103516 Test Loss: 22.114273071289062\n",
      "Epoch: 750 Train Loss: 20.09878158569336 Test Loss: 22.05219078063965\n",
      "Epoch: 760 Train Loss: 20.002065658569336 Test Loss: 22.01690673828125\n",
      "Epoch: 770 Train Loss: 19.895315170288086 Test Loss: 21.94784164428711\n",
      "Epoch: 780 Train Loss: 19.693944931030273 Test Loss: 21.83282470703125\n",
      "Epoch: 790 Train Loss: 19.548616409301758 Test Loss: 21.744657516479492\n",
      "Epoch: 800 Train Loss: 19.616777420043945 Test Loss: 21.73031234741211\n",
      "Epoch: 810 Train Loss: 19.45888328552246 Test Loss: 21.65854263305664\n",
      "Epoch: 820 Train Loss: 19.295616149902344 Test Loss: 21.568105697631836\n",
      "Epoch: 830 Train Loss: 19.19137954711914 Test Loss: 21.495723724365234\n",
      "Epoch: 840 Train Loss: 19.098609924316406 Test Loss: 21.4548397064209\n",
      "Epoch: 850 Train Loss: 19.050724029541016 Test Loss: 21.40099334716797\n",
      "Epoch: 860 Train Loss: 18.977928161621094 Test Loss: 21.339540481567383\n",
      "Epoch: 870 Train Loss: 18.925233840942383 Test Loss: 21.276206970214844\n",
      "Epoch: 880 Train Loss: 18.808738708496094 Test Loss: 21.201250076293945\n",
      "Epoch: 890 Train Loss: 18.724483489990234 Test Loss: 21.160253524780273\n",
      "Epoch: 900 Train Loss: 18.645097732543945 Test Loss: 21.08463478088379\n",
      "Epoch: 910 Train Loss: 18.522645950317383 Test Loss: 20.985088348388672\n",
      "Epoch: 920 Train Loss: 18.42816162109375 Test Loss: 20.9108943939209\n",
      "Epoch: 930 Train Loss: 18.286230087280273 Test Loss: 20.82778549194336\n",
      "Epoch: 940 Train Loss: 18.21906089782715 Test Loss: 20.765583038330078\n",
      "Epoch: 950 Train Loss: 18.146331787109375 Test Loss: 20.68589210510254\n",
      "Epoch: 960 Train Loss: 18.03937339782715 Test Loss: 20.639692306518555\n",
      "Epoch: 970 Train Loss: 17.97927474975586 Test Loss: 20.581188201904297\n",
      "Epoch: 980 Train Loss: 17.924945831298828 Test Loss: 20.509973526000977\n",
      "Epoch: 990 Train Loss: 17.854408264160156 Test Loss: 20.441787719726562\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    layer_1 = relu(train_x @ w_1 + b_1)\n",
    "    pred = layer_1 @ w_2 + b_2\n",
    "    \n",
    "    loss = mean_squared_error(pred, train_y)\n",
    "    \n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w_1 -= w_1.grad * lr\n",
    "        w_2 -= w_2.grad * lr\n",
    "        b_1 -= b_1.grad * lr\n",
    "        b_2 -= b_2.grad * lr\n",
    "        w_1.grad.zero_()\n",
    "        w_2.grad.zero_()\n",
    "        b_1.grad.zero_()\n",
    "        b_2.grad.zero_()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            val_pred = relu(test_x @ w_1 + b_1) @ w_2 + b_2\n",
    "            val_loss = mean_squared_error(val_pred, test_y)\n",
    "            print(f\"Epoch: {epoch} Train Loss: {loss.item()} Test Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
