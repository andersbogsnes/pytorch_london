{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy](images/numpy_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression in plain Numpy\n",
    "Let's start by building a regression from scratch in numpy, so we see what's actually happening behind the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We are using the built-in sklearn dataset Boston House Prices.\n",
    "\n",
    "Our goal is to predict the median price of a home in a given town from a number of features, such as Crime Rate, Property Tax Rate, amount of Industry etc.\n",
    "\n",
    "It's generally a good idea to scale our data, so we use Sklearn's MinMax scaler to scale our values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "boston = load_boston()\n",
    "train_x, test_x, train_y, test_y = train_test_split(boston.data, boston.target, random_state=seed)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "train_y = train_y.reshape(-1, 1)\n",
    "test_y = test_y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters\n",
    "\n",
    "We have some hyperparameters to set, as well as some numbers we need to know upfront.\n",
    "\n",
    "`layer_size` --> We need to know how many input variables there are, so we can create an equivalent number of weights\n",
    "\n",
    "`lr` --> Aka learning rate.\n",
    "When we take a step in our gradient descent, we multiply by this factor, so we don't take too big or too large a step. \n",
    "\n",
    "`epochs` --> How many times should we keep stepping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = train_x.shape[1]\n",
    "lr = 0.1\n",
    "epochs = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights and bias\n",
    "\n",
    "We need one weight to multiply each feature with - we are learning what these should be, so we start them as a random number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(layer_size, 1)\n",
    "b = np.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function\n",
    "\n",
    "Just like before, we want to use mean squared error to say how bad or good our line is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def mean_squared_error(y_hat, y):\n",
    "    return ((y_hat - y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define derivatives\n",
    "In order to find out what size and direction our step should be, we need to get the gradient for each parameter - I've done the math so you don't have to! *(This can be a pain in the behind!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define derivate functions of w and b\n",
    "def w_prime(delta, x):\n",
    "    return np.sum((delta * x), axis=0) / len(x)\n",
    "\n",
    "def b_prime(delta, x):\n",
    "    return np.sum(delta, axis=0) / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 579.0237336860143 Test Loss: 250.7196931414704\n",
      "Epoch: 10 Train Loss: 96.6307912455908 Test Loss: 77.7944162567769\n",
      "Epoch: 20 Train Loss: 75.79565099007046 Test Loss: 60.220490261892046\n",
      "Epoch: 30 Train Loss: 64.2262727847287 Test Loss: 51.0762118062949\n",
      "Epoch: 40 Train Loss: 57.37946374258831 Test Loss: 46.17414236758171\n",
      "Epoch: 50 Train Loss: 52.99067127873133 Test Loss: 43.32607809466068\n",
      "Epoch: 60 Train Loss: 49.91672002798442 Test Loss: 41.47078467736239\n",
      "Epoch: 70 Train Loss: 47.576748878520284 Test Loss: 40.100346334653075\n",
      "Epoch: 80 Train Loss: 45.67230824966247 Test Loss: 38.975598537720536\n",
      "Epoch: 90 Train Loss: 44.047167183658 Test Loss: 37.98629782698098\n",
      "Epoch: 100 Train Loss: 42.61704832422963 Test Loss: 37.08293622270079\n",
      "Epoch: 110 Train Loss: 41.33436956031324 Test Loss: 36.243855514806484\n",
      "Epoch: 120 Train Loss: 40.170515754090296 Test Loss: 35.45964232230221\n",
      "Epoch: 130 Train Loss: 39.10689355499307 Test Loss: 34.72590416968729\n",
      "Epoch: 140 Train Loss: 38.13039082372311 Test Loss: 34.04005466624694\n",
      "Epoch: 150 Train Loss: 37.23104840233451 Test Loss: 33.399976848434704\n",
      "Epoch: 160 Train Loss: 36.400846120979445 Test Loss: 32.80353786518693\n",
      "Epoch: 170 Train Loss: 35.633052596352485 Test Loss: 32.248468652213965\n",
      "Epoch: 180 Train Loss: 34.921862556860404 Test Loss: 31.732384303511807\n",
      "Epoch: 190 Train Loss: 34.262182740172605 Test Loss: 31.252846141969876\n",
      "Epoch: 200 Train Loss: 33.64949621479018 Test Loss: 30.80742512030992\n",
      "Epoch: 210 Train Loss: 33.079769490450474 Test Loss: 30.393752691506613\n",
      "Epoch: 220 Train Loss: 32.54938412398242 Test Loss: 30.009556584651175\n",
      "Epoch: 230 Train Loss: 32.055083266757364 Test Loss: 29.652683208351835\n",
      "Epoch: 240 Train Loss: 31.593928026462724 Test Loss: 29.32110960033521\n",
      "Epoch: 250 Train Loss: 31.163260777084083 Test Loss: 29.012947783877237\n",
      "Epoch: 260 Train Loss: 30.760673722325006 Test Loss: 28.72644390860354\n",
      "Epoch: 270 Train Loss: 30.383981638310626 Test Loss: 28.459973998377897\n",
      "Epoch: 280 Train Loss: 30.031198061804748 Test Loss: 28.21203763618635\n",
      "Epoch: 290 Train Loss: 29.700514386310402 Test Loss: 27.981250522063107\n",
      "Epoch: 300 Train Loss: 29.390281448915218 Test Loss: 27.7663365427992\n",
      "Epoch: 310 Train Loss: 29.098993270384085 Test Loss: 27.566119775701438\n",
      "Epoch: 320 Train Loss: 28.825272667610633 Test Loss: 27.379516695061213\n",
      "Epoch: 330 Train Loss: 28.56785850038549 Test Loss: 27.205528743245328\n",
      "Epoch: 340 Train Loss: 28.325594348463724 Test Loss: 27.043235355559595\n",
      "Epoch: 350 Train Loss: 28.097418442850756 Test Loss: 26.891787479448688\n",
      "Epoch: 360 Train Loss: 27.8823546986702 Test Loss: 26.750401596943853\n",
      "Epoch: 370 Train Loss: 27.67950471692151 Test Loss: 26.618354239326308\n",
      "Epoch: 380 Train Loss: 27.48804063954505 Test Loss: 26.49497697102078\n",
      "Epoch: 390 Train Loss: 27.307198756967146 Test Loss: 26.379651813155167\n",
      "Epoch: 400 Train Loss: 27.136273780061778 Test Loss: 26.271807074199767\n",
      "Epoch: 410 Train Loss: 26.974613699532092 Test Loss: 26.17091355437439\n",
      "Epoch: 420 Train Loss: 26.821615165321884 Test Loss: 26.07648109122672\n",
      "Epoch: 430 Train Loss: 26.676719327017803 Test Loss: 25.9880554153564\n",
      "Epoch: 440 Train Loss: 26.539408083466714 Test Loss: 25.9052152873004\n",
      "Epoch: 450 Train Loss: 26.409200696156454 Test Loss: 25.82756988884722\n",
      "Epoch: 460 Train Loss: 26.285650726417927 Test Loss: 25.754756444349198\n",
      "Epoch: 470 Train Loss: 26.16834326131002 Test Loss: 25.68643804984918\n",
      "Epoch: 480 Train Loss: 26.05689239723987 Test Loss: 25.622301689969664\n",
      "Epoch: 490 Train Loss: 25.950938954031074 Test Loss: 25.562056424498106\n",
      "Epoch: 500 Train Loss: 25.850148395350157 Test Loss: 25.505431728425194\n",
      "Epoch: 510 Train Loss: 25.754208934199088 Test Loss: 25.45217597085193\n",
      "Epoch: 520 Train Loss: 25.66282980462949 Test Loss: 25.40205501968049\n",
      "Epoch: 530 Train Loss: 25.5757396829799 Test Loss: 25.354850960351985\n",
      "Epoch: 540 Train Loss: 25.492685243818123 Test Loss: 25.310360918102013\n",
      "Epoch: 550 Train Loss: 25.413429837422182 Test Loss: 25.268395974284804\n",
      "Epoch: 560 Train Loss: 25.337752277084526 Test Loss: 25.22878016828006\n",
      "Epoch: 570 Train Loss: 25.265445725800113 Test Loss: 25.19134957735592\n",
      "Epoch: 580 Train Loss: 25.19631667302331 Test Loss: 25.155951467626505\n",
      "Epoch: 590 Train Loss: 25.13018399316909 Test Loss: 25.12244350992514\n",
      "Epoch: 600 Train Loss: 25.066878078408728 Test Loss: 25.09069305502149\n",
      "Epoch: 610 Train Loss: 25.006240039082797 Test Loss: 25.06057646315301\n",
      "Epoch: 620 Train Loss: 24.94812096573832 Test Loss: 25.031978483324934\n",
      "Epoch: 630 Train Loss: 24.89238124740231 Test Loss: 25.004791678264\n",
      "Epoch: 640 Train Loss: 24.838889941241618 Test Loss: 24.978915891297515\n",
      "Epoch: 650 Train Loss: 24.787524189236127 Test Loss: 24.954257751774506\n",
      "Epoch: 660 Train Loss: 24.738168677916736 Test Loss: 24.930730215954718\n",
      "Epoch: 670 Train Loss: 24.690715137598076 Test Loss: 24.908252140568745\n",
      "Epoch: 680 Train Loss: 24.645061877872713 Test Loss: 24.88674788650164\n",
      "Epoch: 690 Train Loss: 24.601113356435157 Test Loss: 24.866146950276136\n",
      "Epoch: 700 Train Loss: 24.558779778573413 Test Loss: 24.846383621213537\n",
      "Epoch: 710 Train Loss: 24.51797672490695 Test Loss: 24.82739666233161\n",
      "Epoch: 720 Train Loss: 24.478624805166884 Test Loss: 24.809129013203595\n",
      "Epoch: 730 Train Loss: 24.4406493360084 Test Loss: 24.791527513150196\n",
      "Epoch: 740 Train Loss: 24.403980041020525 Test Loss: 24.774542643271115\n",
      "Epoch: 750 Train Loss: 24.36855077125633 Test Loss: 24.75812828594436\n",
      "Epoch: 760 Train Loss: 24.334299244748383 Test Loss: 24.74224150053197\n",
      "Epoch: 770 Train Loss: 24.301166803603497 Test Loss: 24.726842314131382\n",
      "Epoch: 780 Train Loss: 24.269098187386614 Test Loss: 24.711893526303168\n",
      "Epoch: 790 Train Loss: 24.238041321609483 Test Loss: 24.6973605267888\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    pred = train_x @ w + b\n",
    "    loss = mean_squared_error(pred, train_y)\n",
    "    \n",
    "    # Backpropagation\n",
    "    delta = pred - train_y\n",
    "    w -= w_prime(delta, train_x).reshape(-1, 1) * lr\n",
    "    b -= b_prime(delta, train_x) * lr\n",
    "    \n",
    "    # Validate model\n",
    "    if epoch % 10 == 0:\n",
    "        val_pred = test_x @ w + b\n",
    "        val_loss = mean_squared_error(val_pred, test_y)\n",
    "        print(f\"Epoch: {epoch} Train Loss: {loss} Test Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to Sklearn\n",
    "\n",
    "Just to make sure we've not done something horribly wrong, let's compare our homemade Gradient Descent Linear Regression vs the sklearn LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression = LinearRegression()\n",
    "regression.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.098694827098"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error(test_y, regression.predict(test_x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
