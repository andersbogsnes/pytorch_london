{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pytorch](images/pytorch_logo.png)\n",
    "# Regression in Pytorch with all the bells and whistles\n",
    "Pytorch lets us skip a bunch of math, but that's only part of why it's heavily used in Deep Learning. It is after all a Deep Learning framework, not a maths framework!\n",
    "\n",
    "Let's redo our simple Linear Regression, but this time we use all the bells and whistles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We are using the built-in sklearn dataset Boston House Prices.\n",
    "\n",
    "Our goal is to predict the median price of a home in a given town from a number of features, such as Crime Rate, Property Tax Rate, amount of Industry etc.\n",
    "\n",
    "It's generally a good idea to scale our data, so we use Sklearn's MinMax scaler to scale our values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "boston = load_boston()\n",
    "train_x, test_x, train_y, test_y = train_test_split(boston.data, boston.target, random_state=seed)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "train_x = torch.tensor(scaler.fit_transform(train_x), dtype=torch.float)\n",
    "test_x = torch.tensor(scaler.transform(test_x), dtype=torch.float)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float).view(-1, 1)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup parameters\n",
    "\n",
    "We have some hyperparameters to set, as well as some numbers we need to know upfront.\n",
    "\n",
    "`layer_size` --> We need to know how many input variables there are, so we can create an equivalent number of weights\n",
    "\n",
    "`lr` --> Aka learning rate.\n",
    "When we take a step in our gradient descent, we multiply by this factor, so we don't take too big or too large a step. \n",
    "\n",
    "`epochs` --> How many times should we keep stepping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "layer_size = train_x.shape[1]\n",
    "lr = 0.05\n",
    "epochs = 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model\n",
    "We get a new toy here:\n",
    "\n",
    "- `nn.Linear`\n",
    "\n",
    "The linear class basically implements all that stuff we were doing by hand before, of setting up and keeping track of the `w` and `b` variables. This includes some initialization best practices, so we get that for free!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function and optimizer\n",
    "Pytorch predefines a number of loss functions for me, so I can just use those directly.\n",
    "Another new features is the SGD optimizer. Until now we have been updating the weights \"by hand\" in a fairly naive fashion. There are many ways of updating our weights and Pytorch provides implementations for most of these. SGD is the closest to what we've been doing so far, so we use that to handle our weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(layer_size, 1) # Defines our parameters for us\n",
    "loss_func = nn.MSELoss() # Define loss func\n",
    "opt = optim.SGD(model.parameters(), lr=lr) # Tell the optimizer what parameters to keep track of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 624.5175170898438 Test Loss: 260.6249694824219\n",
      "Epoch: 10 Training Loss: 96.25338745117188 Test Loss: 75.32975769042969\n",
      "Epoch: 20 Training Loss: 75.75519561767578 Test Loss: 58.45773696899414\n",
      "Epoch: 30 Training Loss: 64.2809066772461 Test Loss: 49.66794204711914\n",
      "Epoch: 40 Training Loss: 57.41917037963867 Test Loss: 44.96377944946289\n",
      "Epoch: 50 Training Loss: 52.97154235839844 Test Loss: 42.2337646484375\n",
      "Epoch: 60 Training Loss: 49.82622528076172 Test Loss: 40.45537567138672\n",
      "Epoch: 70 Training Loss: 47.4166145324707 Test Loss: 39.140419006347656\n",
      "Epoch: 80 Training Loss: 45.45014572143555 Test Loss: 38.05980682373047\n",
      "Epoch: 90 Training Loss: 43.772483825683594 Test Loss: 37.108489990234375\n",
      "Epoch: 100 Training Loss: 42.29939270019531 Test Loss: 36.23963165283203\n",
      "Epoch: 110 Training Loss: 40.9825325012207 Test Loss: 35.432945251464844\n",
      "Epoch: 120 Training Loss: 39.792274475097656 Test Loss: 34.67971420288086\n",
      "Epoch: 130 Training Loss: 38.70893859863281 Test Loss: 33.97587203979492\n",
      "Epoch: 140 Training Loss: 37.718406677246094 Test Loss: 33.319000244140625\n",
      "Epoch: 150 Training Loss: 36.809814453125 Test Loss: 32.70703125\n",
      "Epoch: 160 Training Loss: 35.974334716796875 Test Loss: 32.13783264160156\n",
      "Epoch: 170 Training Loss: 35.20455551147461 Test Loss: 31.609130859375\n",
      "Epoch: 180 Training Loss: 34.49407196044922 Test Loss: 31.118518829345703\n",
      "Epoch: 190 Training Loss: 33.83729553222656 Test Loss: 30.66353416442871\n",
      "Epoch: 200 Training Loss: 33.22928237915039 Test Loss: 30.241731643676758\n",
      "Epoch: 210 Training Loss: 32.66563034057617 Test Loss: 29.850736618041992\n",
      "Epoch: 220 Training Loss: 32.142425537109375 Test Loss: 29.488271713256836\n",
      "Epoch: 230 Training Loss: 31.656156539916992 Test Loss: 29.152191162109375\n",
      "Epoch: 240 Training Loss: 31.203662872314453 Test Loss: 28.840478897094727\n",
      "Epoch: 250 Training Loss: 30.782114028930664 Test Loss: 28.551259994506836\n",
      "Epoch: 260 Training Loss: 30.388948440551758 Test Loss: 28.28280258178711\n",
      "Epoch: 270 Training Loss: 30.021862030029297 Test Loss: 28.033506393432617\n",
      "Epoch: 280 Training Loss: 29.67876625061035 Test Loss: 27.801895141601562\n",
      "Epoch: 290 Training Loss: 29.35776710510254 Test Loss: 27.58661460876465\n",
      "Epoch: 300 Training Loss: 29.057153701782227 Test Loss: 27.38642120361328\n",
      "Epoch: 310 Training Loss: 28.775362014770508 Test Loss: 27.200164794921875\n",
      "Epoch: 320 Training Loss: 28.51097297668457 Test Loss: 27.02680015563965\n",
      "Epoch: 330 Training Loss: 28.262691497802734 Test Loss: 26.8653507232666\n",
      "Epoch: 340 Training Loss: 28.029333114624023 Test Loss: 26.714937210083008\n",
      "Epoch: 350 Training Loss: 27.80982208251953 Test Loss: 26.57473373413086\n",
      "Epoch: 360 Training Loss: 27.603166580200195 Test Loss: 26.443994522094727\n",
      "Epoch: 370 Training Loss: 27.408458709716797 Test Loss: 26.322023391723633\n",
      "Epoch: 380 Training Loss: 27.22486114501953 Test Loss: 26.20818519592285\n",
      "Epoch: 390 Training Loss: 27.051612854003906 Test Loss: 26.101882934570312\n",
      "Epoch: 400 Training Loss: 26.88800811767578 Test Loss: 26.00257682800293\n",
      "Epoch: 410 Training Loss: 26.733400344848633 Test Loss: 25.909765243530273\n",
      "Epoch: 420 Training Loss: 26.587181091308594 Test Loss: 25.822975158691406\n",
      "Epoch: 430 Training Loss: 26.44880485534668 Test Loss: 25.741790771484375\n",
      "Epoch: 440 Training Loss: 26.317758560180664 Test Loss: 25.665800094604492\n",
      "Epoch: 450 Training Loss: 26.193567276000977 Test Loss: 25.594642639160156\n",
      "Epoch: 460 Training Loss: 26.07579231262207 Test Loss: 25.527969360351562\n",
      "Epoch: 470 Training Loss: 25.964025497436523 Test Loss: 25.46547508239746\n",
      "Epoch: 480 Training Loss: 25.857892990112305 Test Loss: 25.40685272216797\n",
      "Epoch: 490 Training Loss: 25.75704002380371 Test Loss: 25.351835250854492\n",
      "Epoch: 500 Training Loss: 25.661142349243164 Test Loss: 25.300167083740234\n",
      "Epoch: 510 Training Loss: 25.569894790649414 Test Loss: 25.25161361694336\n",
      "Epoch: 520 Training Loss: 25.483020782470703 Test Loss: 25.205955505371094\n",
      "Epoch: 530 Training Loss: 25.400251388549805 Test Loss: 25.16299057006836\n",
      "Epoch: 540 Training Loss: 25.321340560913086 Test Loss: 25.122526168823242\n",
      "Epoch: 550 Training Loss: 25.246065139770508 Test Loss: 25.084388732910156\n",
      "Epoch: 560 Training Loss: 25.174205780029297 Test Loss: 25.048412322998047\n",
      "Epoch: 570 Training Loss: 25.105567932128906 Test Loss: 25.014448165893555\n",
      "Epoch: 580 Training Loss: 25.039960861206055 Test Loss: 24.98234748840332\n",
      "Epoch: 590 Training Loss: 24.97721290588379 Test Loss: 24.951984405517578\n",
      "Epoch: 600 Training Loss: 24.917160034179688 Test Loss: 24.92322540283203\n",
      "Epoch: 610 Training Loss: 24.859651565551758 Test Loss: 24.89596939086914\n",
      "Epoch: 620 Training Loss: 24.804542541503906 Test Loss: 24.870101928710938\n",
      "Epoch: 630 Training Loss: 24.751697540283203 Test Loss: 24.845521926879883\n",
      "Epoch: 640 Training Loss: 24.70099639892578 Test Loss: 24.8221378326416\n",
      "Epoch: 650 Training Loss: 24.65231704711914 Test Loss: 24.799869537353516\n",
      "Epoch: 660 Training Loss: 24.605548858642578 Test Loss: 24.778629302978516\n",
      "Epoch: 670 Training Loss: 24.560590744018555 Test Loss: 24.75834083557129\n",
      "Epoch: 680 Training Loss: 24.517343521118164 Test Loss: 24.738935470581055\n",
      "Epoch: 690 Training Loss: 24.475719451904297 Test Loss: 24.720355987548828\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    pred = model(train_x)\n",
    "    loss = loss_func(pred, train_y)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward() # The magic bit\n",
    "    opt.step() # A new magic bit\n",
    "    opt.zero_grad() # Gotta reset the gradients to zero, so they don't accumulate\n",
    "    \n",
    "    # Validate model\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(test_x)\n",
    "        val_loss = loss_func(val_pred, test_y) # Calculate validation loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} Training Loss: {loss.item()} Test Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
